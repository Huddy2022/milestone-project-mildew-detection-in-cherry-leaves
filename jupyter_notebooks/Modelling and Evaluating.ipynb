{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "0aStgWSO0E0E"
            },
            "source": [
                "# **Modeling and Evaluation Notebook**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "1eLEkw5O0ECa"
            },
            "source": [
                "## Objectives\n",
                "\n",
                "* Answer business requirement 2:\n",
                "  * The client is interested in prediciting if a cherry leaf is healthy or contains powdery mildew.\n",
                "\n",
                "## Inputs\n",
                "\n",
                "* inputs/cherry-leaves_dataset/cherry-leaves/train\n",
                "* inputs/cherry-leaves_dataset/cherry-leaves/test\n",
                "* inputs/cherry-leaves_dataset/cherry-leaves/validation\n",
                "* image shape embeddings (pickle file)\n",
                "\n",
                "## Outputs\n",
                "\n",
                "* Images distribution plot in train, validation, and test set\n",
                "  * Label distribution in a bar chart\n",
                "  * Dataset distribution in a pie chart\n",
                "* Image augmentation\n",
                "* Class indices to change prediction inference in labels\n",
                "* TensorFlow convolutional network machine learning model\n",
                "* Hyperparameter optimisation pipeline\n",
                "* Model trained on best hyperparameters as generated by Keras Tuner\n",
                "* Save trained model\n",
                "* Learning curve plot for model performance\n",
                "* Model evaluation on pickle file\n",
                "* Calculate classification report (Model A)\n",
                "* Calculate classification report with macro avg and weighted avg (Model B)\n",
                "* Prediction on the random image file\n",
                "\n",
                "## Comments/Insights/Conclusions\n",
                "\n",
                "* The same data was plotted in different versions to accomodate possible client's requests of further data understanding, and saved in outputs V1 - V5.\n",
                "* The CNN was built to seek the maximum accuracy while minimizing loss and training time, various models/hyperparameters were chosen.\n",
                "* The CNN was kept as small as possible withouth compromising accuracy and avoiding overfitting or underfitting.\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "9uWZXH9LwoQg"
            },
            "source": [
                "---"
            ]
        },
        {
            "source": [
                "## Import packages"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from matplotlib.image import imread"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cqP-UeN-z3i2"
            },
            "source": [
                "# Change working directory"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "aOGIGS-uz3i2"
            },
            "source": [
                "We need to change the working directory from its current folder to its parent folder\n",
                "* We access the current directory with os.getcwd()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "id": "wZfF_j-Bz3i4",
                "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": "'/workspaces/milestone-project-mildew-detection-in-cherry-leaves/jupyter_notebooks'"
                    },
                    "metadata": {},
                    "execution_count": 2
                }
            ],
            "source": [
                "import os\n",
                "current_dir = os.getcwd()\n",
                "current_dir"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "9MWW8E7lz3i7"
            },
            "source": [
                "We want to make the parent of the current directory the new current directory\n",
                "* os.path.dirname() gets the parent directory\n",
                "* os.chir() defines the new current directory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "id": "TwHsQRWjz3i9",
                "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "You set a new current directory\n"
                }
            ],
            "source": [
                "os.chdir(os.path.dirname(current_dir))\n",
                "print(\"You set a new current directory\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "M_xPk_Ijz3i-"
            },
            "source": [
                "Confirm the new current directory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "id": "vz3S-_kjz3jA",
                "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": "'/workspaces/milestone-project-mildew-detection-in-cherry-leaves'"
                    },
                    "metadata": {},
                    "execution_count": 4
                }
            ],
            "source": [
                "current_dir = os.getcwd()\n",
                "current_dir"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "-mavJ8DibrcQ"
            },
            "source": [
                "## Set input directories"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Set train, validation and test paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "my_data_dir = 'inputs/cherry-leaves_dataset/cherry-leaves'\n",
                "train_path = my_data_dir + '/train'\n",
                "val_path = my_data_dir + '/validation'\n",
                "test_path = my_data_dir + '/test'"
            ]
        },
        {
            "source": [
                "## Set output directory"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Old version is already available create a new version.\n"
                }
            ],
            "source": [
                "version = 'v1'\n",
                "file_path = f'outputs/{version}'\n",
                "\n",
                "if 'outputs' in os.listdir(current_dir) and version in os.listdir(current_dir + '/outputs'):\n",
                "  print('Old version is already available create a new version.')\n",
                "  pass\n",
                "else:\n",
                "  os.makedirs(name=file_path)"
            ]
        },
        {
            "source": [
                "## Set labels"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Project Labels: ['healthy', 'powdery_mildew']\n"
                }
            ],
            "source": [
                "labels = os.listdir(train_path)\n",
                "\n",
                "print(\n",
                "    f\"Project Labels: {labels}\"\n",
                "    )"
            ]
        },
        {
            "source": [
                "## Set image shape"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": "(256, 256, 3)"
                    },
                    "metadata": {},
                    "execution_count": 8
                }
            ],
            "source": [
                "## Import saved image shape embedding\n",
                "import joblib\n",
                "version = 'v1'\n",
                "image_shape = joblib.load(filename=f\"outputs/{version}/image_shape.pkl\")\n",
                "image_shape"
            ]
        },
        {
            "source": [
                "---"
            ],
            "cell_type": "markdown",
            "metadata": {
                "id": "ZY3l0-AxO93d"
            }
        },
        {
            "source": [
                "# Number of images in train, test and validation data"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "Count number of images per label & set"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "* train - healthy: 1472 images\n* train - powdery_mildew: 1472 images\n* validation - healthy: 210 images\n* validation - powdery_mildew: 210 images\n* test - healthy: 422 images\n* test - powdery_mildew: 422 images\n   Frequency           Label         Set\n0     1472.0         healthy       train\n1     1472.0  powdery_mildew       train\n2      210.0         healthy  validation\n3      210.0  powdery_mildew  validation\n4      422.0         healthy        test\n5      422.0  powdery_mildew        test\n"
                }
            ],
            "source": [
                "df_freq = pd.DataFrame([]) \n",
                "for folder in ['train', 'validation', 'test']:\n",
                "  for label in labels:\n",
                "    df_freq = df_freq.append(\n",
                "        pd.Series(data={'Set': folder,\n",
                "                        'Label': label,\n",
                "                        'Frequency':int(len(os.listdir(my_data_dir + '/' + folder + '/' + label)))}\n",
                "                  ),\n",
                "                  ignore_index=True\n",
                "        )\n",
                "    print(f\"* {folder} - {label}: {len(os.listdir(my_data_dir+'/'+ folder + '/' + label))} images\")\n",
                "\n",
                "print(df_freq)"
            ]
        },
        {
            "source": [
                "## Bar chart for label distribution"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sns.set_style(\"whitegrid\")\n",
                "plt.figure(figsize=(8,5))\n",
                "ax = sns.barplot(data=df_freq, x='Set', y='Frequency', hue='Label')\n",
                "\n",
                "# Add labels to the bars\n",
                "for p in ax.patches:\n",
                "    height = p.get_height()\n",
                "    ax.text(p.get_x() + p.get_width() / 2, height, f'{height}', ha='center', va='bottom')\n",
                "\n",
                "plt.legend(loc='upper right')\n",
                "plt.title('Cherry leaves Label distribution')\n",
                "plt.savefig(f'{file_path}/labels_distribution.png', bbox_inches='tight', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "source": [
                "## Pie chart for dataset distribution"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(8,5))\n",
                "set_labels = df_freq['Set'].unique()\n",
                "colors = sns.color_palette('pastel')[:len(set_labels)]\n",
                "explode = [0.1] * len(set_labels)\n",
                "\n",
                "set_frequencies = []\n",
                "for set_label in set_labels:\n",
                "    set_frequencies.append(df_freq[df_freq['Set'] == set_label]['Frequency'].sum())\n",
                "\n",
                "plt.pie(set_frequencies, labels=set_labels, colors=colors, explode=explode, autopct='%.0f%%')\n",
                "plt.title('Cherry leaves dataset distribution')\n",
                "plt.savefig(f'{file_path}/sets_distribution_pie.png',\n",
                "            bbox_inches='tight', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "uFQo3ycuO-v6"
            },
            "source": [
                "----"
            ]
        },
        {
            "source": [
                "# Image data augmentation"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                " Our dataset only shows a limited number of images, we need to train the model by augmenting the images to better improve the ML performance"
            ]
        },
        {
            "source": [
                "## Import ImageDataGenerator form Tensorflow"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
            ]
        },
        {
            "source": [
                "## Intiatize ImageDataGenerator"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "augmented_image_data = ImageDataGenerator(rotation_range=20,\n",
                "                                   width_shift_range=0.10, \n",
                "                                   height_shift_range=0.10,\n",
                "                                   shear_range=0.1,\n",
                "                                   zoom_range=0.1,\n",
                "                                   horizontal_flip=True,\n",
                "                                   vertical_flip=True,\n",
                "                                   fill_mode='nearest',\n",
                "                                   rescale=1./255\n",
                "                              )"
            ]
        },
        {
            "source": [
                "## Augment training image dataset"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "batch_size = 20 # Set batch size\n",
                "train_set = augmented_image_data.flow_from_directory(train_path,\n",
                "                                              target_size=image_shape[:2],\n",
                "                                              color_mode='rgb',\n",
                "                                              batch_size=batch_size,\n",
                "                                              class_mode='binary',\n",
                "                                              shuffle=True\n",
                "                                              )\n",
                "\n",
                "train_set.class_indices"
            ]
        },
        {
            "source": [
                "## Augment validation image dataset"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "validation_set = ImageDataGenerator(rescale=1./255).flow_from_directory(val_path,\n",
                "                                                          target_size=image_shape[:2],\n",
                "                                                          color_mode='rgb',\n",
                "                                                          batch_size=batch_size,\n",
                "                                                          class_mode='binary',\n",
                "                                                          shuffle=False\n",
                "                                                          )\n",
                "\n",
                "validation_set.class_indices"
            ]
        },
        {
            "source": [
                "## Augment test image dataset"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "test_set = ImageDataGenerator(rescale=1./255).flow_from_directory(test_path,\n",
                "                                                    target_size=image_shape[:2],\n",
                "                                                    color_mode='rgb',\n",
                "                                                    batch_size=batch_size,\n",
                "                                                    class_mode='binary',\n",
                "                                                    shuffle=False\n",
                "                                                    )\n",
                "\n",
                "test_set.class_indices"
            ]
        },
        {
            "source": [
                "## Plot augmented training image"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "\n",
                "for _ in range(3):\n",
                "    plt.figure(figsize=(3, 3))\n",
                "    img, label = train_set.next()\n",
                "    print(img.shape)   #  (1,256,256,3)\n",
                "    plt.imshow(img[0])\n",
                "    plt.show()"
            ]
        },
        {
            "source": [
                "## Plot augmented validation image"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "for _ in range(3):\n",
                "    plt.figure(figsize=(3, 3))\n",
                "    img, label = validation_set.next()\n",
                "    print(img.shape)   #  (1,256,256,3)\n",
                "    plt.imshow(img[0])\n",
                "    plt.show()"
            ]
        },
        {
            "source": [
                "## Plot augmented test image"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "for _ in range(3):\n",
                "    plt.figure(figsize=(3, 3))\n",
                "    img, label = test_set.next()\n",
                "    print(img.shape)   #  (1,256,256,3)\n",
                "    plt.imshow(img[0])\n",
                "    plt.show()"
            ]
        },
        {
            "source": [
                "## Save class_indices"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "joblib.dump(value=train_set.class_indices ,\n",
                "            filename=f\"{file_path}/class_indices.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "source": [
                "# Model creation"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "## Install and import the Keras Tuner for hyperparameter optimisation"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "!pip install keras-tuner"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "import keras_tuner as kt"
            ]
        },
        {
            "source": [
                "## Import model packages"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D"
            ]
        },
        {
            "source": [
                "## Contsruct function to create hypermodel for hypertuning"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_tf_model(hp):\n",
                "    model = Sequential()\n",
                "\n",
                "    model.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=image_shape, activation='relu',))\n",
                "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
                "\n",
                "    model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape, activation='relu',))\n",
                "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
                "\n",
                "    model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape, activation='relu',))\n",
                "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
                "\n",
                "    model.add(Flatten())\n",
                "    hp_units = hp.Int('units', min_value=128, max_value=256, step=32)\n",
                "    model.add(Dense(units=hp_units, activation = 'relu'))\n",
                "\n",
                "    model.add(Dropout(0.5))\n",
                "    model.add(Dense(1, activation = 'sigmoid'))\n",
                "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n",
                "\n",
                "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
                "                loss='binary_crossentropy',\n",
                "                metrics=['accuracy'])\n",
                "    \n",
                "    return model"
            ]
        },
        {
            "source": [
                "## Instantiate the tuner and perform hypertuning"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "tuner = kt.Hyperband(create_tf_model,\n",
                "                     objective='val_accuracy',\n",
                "                     max_epochs=10,\n",
                "                     factor=3,\n",
                "                     directory=file_path,\n",
                "                     project_name='hypertuning')"
            ]
        },
        {
            "source": [
                "## Early Stopping"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.callbacks import EarlyStopping\n",
                "early_stop = EarlyStopping(monitor='val_loss',patience=3)"
            ]
        },
        {
            "source": [
                "## Run the hyperparameter search"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "from tensorflow.keras.callbacks import EarlyStopping\n",
                "early_stop = EarlyStopping(monitor='val_loss',patience=3)\n",
                "\n",
                "tuner.search(train_set,\n",
                "          epochs=25,\n",
                "          steps_per_epoch = len(train_set.classes) // batch_size,\n",
                "          validation_data=validation_set,\n",
                "          callbacks=[early_stop],\n",
                "          verbose=1\n",
                "          )\n",
                "\n",
                "# Get the optimal hyperparameters\n",
                "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
                "\n",
                "print(f\"\"\"\n",
                "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
                "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
                "is {best_hps.get('learning_rate')}.\n",
                "\"\"\")"
            ]
        },
        {
            "source": [
                "## Create model using defined best hyperparameters"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = tuner.hypermodel.build(best_hps)"
            ]
        },
        {
            "source": [
                "## Model Summary"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "model.summary()"
            ]
        },
        {
            "source": [
                "## Re-instantiate the hypermodel and train it"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = tuner.hypermodel.build(best_hps)\n",
                "\n",
                "# Retrain the model\n",
                "model.fit(train_set,\n",
                "          epochs=25,\n",
                "          steps_per_epoch = len(train_set.classes) // batch_size,\n",
                "          validation_data=validation_set,\n",
                "          class_weight=train_class_weights,\n",
                "          callbacks=[early_stop],\n",
                "          verbose=1\n",
                "          )"
            ]
        },
        {
            "source": [
                "## Save the model"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.save('outputs/v1/powdery_mildew_detection_model.h5')\n"
            ]
        },
        {
            "source": [
                "----"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "# Model Performace"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "----"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "## Model learning curve"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "losses = pd.DataFrame(model.history.history)\n",
                "\n",
                "sns.set_style(\"whitegrid\")\n",
                "losses[['loss','val_loss']].plot(style='.-')\n",
                "plt.title(\"Loss\")\n",
                "plt.savefig(f'{file_path}/model_training_losses.png', bbox_inches='tight', dpi=150)\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n\")\n",
                "losses[['accuracy','val_accuracy']].plot(style='.-')\n",
                "plt.title(\"Accuracy\")\n",
                "plt.savefig(f'{file_path}/model_training_acc.png', bbox_inches='tight', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "source": [
                "# Model Evaluation"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "## Load saved model"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from keras.models import load_model\n",
                "model = load_model('outputs/v1/powdery_mildew_detection_model.h5')"
            ]
        },
        {
            "source": [
                "## Evaluate model on test set"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "evaluation = model.evaluate(test_set)\n",
                "print(\"Model accuracy: {:.2f}%\".format(evaluation[1] * 100))\n",
                "print(\"Model Loss: \",evaluation[0])"
            ]
        },
        {
            "source": [
                "## Save evaluation pickle"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "joblib.dump(value=evaluation ,\n",
                "            filename=f\"outputs/v1/evaluation.pkl\")"
            ]
        },
        {
            "source": [
                "## Confusion Matrix"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import confusion_matrix\n",
                "\n",
                "# Predict labels\n",
                "pred = model.predict(test_set)\n",
                "y_pred = np.concatenate(np.round(pred).astype(int))\n",
                "\n",
                "# Define target names\n",
                "target_names = ['Powdery Mildew', 'Healthy']\n",
                "\n",
                "# Compute confusion matrix\n",
                "cm = confusion_matrix(test_set.classes, y_pred)\n",
                "\n",
                "# Create a figure and axis\n",
                "fig, ax = plt.subplots(figsize=(7, 6))\n",
                "\n",
                "# Plot the heatmap\n",
                "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',\n",
                "            xticklabels=target_names, yticklabels=target_names,\n",
                "            ax=ax)\n",
                "\n",
                "# Set labels and title\n",
                "ax.set_xlabel('Predicted')\n",
                "ax.set_ylabel('Actual')\n",
                "ax.set_title('Confusion Matrix')\n",
                "\n",
                "# Save the figure\n",
                "plt.savefig(f'{file_path}/confusion_matrix.png', bbox_inches='tight', dpi=150)"
            ]
        },
        {
            "source": [
                "## Classification Report - A"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "from sklearn.metrics import classification_report\n",
                "\n",
                "print('Classification Report:\\n----------------------\\n')\n",
                "print(classification_report(test_set.classes, y_pred, \n",
                "      target_names=target_names))\n"
            ]
        },
        {
            "source": [
                "## Classification Report - B"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "classification_rep = classification_report(test_set.classes, y_pred, target_names=target_names, output_dict=True)\n",
                "\n",
                "# Print the keys in the classification_rep dictionary\n",
                "print(classification_rep.keys())\n",
                "\n",
                "# Define the target class labels\n",
                "target_classes = ['accuracy', 'Healthy', 'Powdery Mildew', 'macro avg', 'weighted avg']\n",
                "\n",
                "# Extract the required metrics\n",
                "metric_labels = ['precision', 'recall', 'f1-score']\n",
                "\n",
                "# Create the data matrix\n",
                "data = np.zeros((len(target_classes), len(metric_labels)))\n",
                "for i, class_label in enumerate(target_classes):\n",
                "    if class_label == 'accuracy':\n",
                "        data[i, :] = round(classification_rep[class_label], 3)  # Assign the accuracy value directly to all metrics\n",
                "    else:\n",
                "        for j, metric_label in enumerate(metric_labels):\n",
                "            data[i, j] = round(classification_rep[class_label][metric_label], 3)  # Round the values to 3 decimal places\n",
                "\n",
                "# Plot the heatmap\n",
                "plt.figure(figsize=(8, 5))\n",
                "sns.heatmap(data, annot=True, fmt='.3f', cmap=\"Blues\", cbar=True, linewidths=1)  # Use fmt='.3f' to display values with 3 decimal places and cbar=True to show the color bar\n",
                "plt.xticks(np.arange(len(metric_labels)) + 0.5, metric_labels, rotation=0)\n",
                "plt.yticks(np.arange(len(target_classes)) + 0.5, target_classes, rotation=0)  # Set the class labels on the y-axis\n",
                "plt.xlabel('Metric')\n",
                "plt.ylabel('Class')\n",
                "plt.title('Classification Report')\n",
                "plt.savefig(f'{file_path}/clf_report.png', bbox_inches='tight', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "source": [
                "## Test prediction using a random image"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "import random\n",
                "from tensorflow.keras.preprocessing import image\n",
                "\n",
                "label = labels[0] # select healthy or infected\n",
                "\n",
                "# Get the list of image file names for the specified label category\n",
                "image_files = os.listdir(test_path + '/' + label)\n",
                "\n",
                "# Select a random image file\n",
                "random_image_file = random.choice(image_files)\n",
                "\n",
                "# Load and resize the random image\n",
                "pil_image = image.load_img(test_path + '/'+ label + '/'+ random_image_file,\n",
                "                          target_size=image_shape, color_mode='rgb')\n",
                "\n",
                "print(f'Image shape: {pil_image.size}, Image mode: {pil_image.mode}')\n",
                "pil_image"
            ]
        },
        {
            "source": [
                "## Convert image to array and prepare for prediction"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "my_image = image.img_to_array(pil_image)\n",
                "my_image = np.expand_dims(my_image, axis=0)/255\n",
                "print(my_image.shape)"
            ]
        },
        {
            "source": [
                "## Predict class probabilities"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "pred_proba = model.predict(my_image)[0,0]\n",
                "\n",
                "target_map = {v: k for k, v in train_set.class_indices.items()}\n",
                "pred_class =  target_map[pred_proba > 0.5]  \n",
                "\n",
                "if pred_class == target_map[0]:\n",
                "    pred_proba = 1 - pred_proba\n",
                "\n",
                "pred_percentage = round(pred_proba * 100, 2)\n",
                "\n",
                "print(f\"Label: {pred_class}\")\n",
                "print(f\"Percentage: {pred_percentage}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "source": [
                "# Conclusion\n",
                "\n",
                "* We have counted the number of labels/sets and displayed the labels and dataset distribution in bar and pie charts respectively.\n",
                "* The images have been augmented to better train the model to better improve the ML performance.\n",
                "* Plotted the images in each set and save them to a pickle file.\n",
                "* Installed Keras Tuner for hyperparameter optimisation.\n",
                "* Ran a hyperparameter search to find the best hyperparameters and retrained the hypermodel with these hyperparameters before saving the model to outputs.\n",
                "* Plotted the performance of the model to better understand the loss and accuracy.\n",
                "* Evaluated the model and save it to a pickle file.\n",
                "* Ran a confusion matrix for the model performance\n",
                "* Created two classification reports to also show the model performance.\n",
                "* Lastly tested the prediction and predicted its class probabilities. "
            ],
            "cell_type": "markdown",
            "metadata": {}
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "name": "Data Practitioner Jupyter Notebook.ipynb",
            "provenance": [],
            "toc_visible": true
        },
        "interpreter": {
            "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.12-final"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}